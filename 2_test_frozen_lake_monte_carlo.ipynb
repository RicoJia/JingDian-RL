{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rico: current state 14, new state: 15\n",
      "Rico: current state 55, new state: 63\n",
      "Rico: current state 62, new state: 63\n",
      "\n",
      "-------------------------\n",
      "Beginning Policy Iteration\n",
      "-------------------------\n",
      "======================0======================\n",
      "\n",
      "Policy: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "value function: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "======================1======================\n",
      "\n",
      "Policy: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "value function: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "======================2======================\n",
      "\n",
      "Policy: [0 0 0 0 0 0 0 0 0 0 1 0 0 2 2 0]\n",
      "value function: [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.9 0.  0.  0.9 1.  0. ]\n",
      "======================3======================\n",
      "\n",
      "Policy: [0 0 0 0 0 0 1 0 0 1 1 0 0 2 2 0]\n",
      "value function: [0.   0.   0.   0.   0.   0.   0.81 0.   0.   0.81 0.9  0.   0.   0.9\n",
      " 1.   0.  ]\n",
      "======================4======================\n",
      "\n",
      "Policy: [0 0 1 0 0 0 1 0 2 1 1 0 0 2 2 0]\n",
      "value function: [0.    0.    0.729 0.656 0.    0.    0.81  0.    0.729 0.81  0.9   0.\n",
      " 0.    0.9   1.    0.   ]\n",
      "======================5======================\n",
      "\n",
      "Policy: [0 2 1 0 1 0 1 0 2 1 1 0 0 2 2 0]\n",
      "value function: [0.    0.656 0.729 0.656 0.656 0.    0.81  0.    0.729 0.81  0.9   0.\n",
      " 0.    0.9   1.    0.   ]\n",
      "======================6======================\n",
      "\n",
      "Policy: [1 2 1 0 1 0 1 0 2 1 1 0 0 2 2 0]\n",
      "value function: [0.59  0.656 0.729 0.656 0.656 0.    0.81  0.    0.729 0.81  0.9   0.\n",
      " 0.    0.9   1.    0.   ]\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Episode reward: 1.000000\n",
      "\n",
      "-------------------------\n",
      "Beginning Value Iteration\n",
      "-------------------------\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Episode reward: 1.000000\n",
      "\n",
      "-------------------------\n",
      "Beginning Policy Iteration\n",
      "-------------------------\n",
      "======================0======================\n",
      "\n",
      "Policy: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "value function: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "======================1======================\n",
      "\n",
      "Policy: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 2 0]\n",
      "value function: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "======================2======================\n",
      "\n",
      "Policy: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 2 2 0]\n",
      "value function: [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.9 0.  0.  0.  0.  0.  0.\n",
      " 0.  1.  0.  0.  0.  0.  0.  0.9 1.  0. ]\n",
      "======================3======================\n",
      "\n",
      "Policy: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 2 2 2 0]\n",
      "value function: [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.81 0.   0.\n",
      " 0.   0.   0.   0.   0.   0.9  0.   0.   0.   0.   0.   0.81 0.   1.\n",
      " 0.   0.   0.   0.   0.81 0.9  1.   0.  ]\n",
      "======================4======================\n",
      "\n",
      "Policy: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 2 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 2 2 2 0]\n",
      "value function: [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.729 0.    0.    0.    0.\n",
      " 0.    0.    0.729 0.81  0.    0.    0.    0.    0.    0.729 0.    0.9\n",
      " 0.    0.    0.    0.    0.    0.81  0.    1.    0.    0.    0.    0.\n",
      " 0.81  0.9   1.    0.   ]\n",
      "======================5======================\n",
      "\n",
      "Policy: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0\n",
      " 1 2 1 0 0 0 0 2 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 2 2 2 0]\n",
      "value function: [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.656\n",
      " 0.    0.    0.    0.    0.    0.    0.656 0.729 0.    0.    0.    0.\n",
      " 0.    0.656 0.729 0.81  0.    0.    0.    0.    0.656 0.729 0.    0.9\n",
      " 0.    0.    0.    0.    0.    0.81  0.    1.    0.    0.    0.    0.\n",
      " 0.81  0.9   1.    0.   ]\n",
      "======================6======================\n",
      "\n",
      "Policy: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 1\n",
      " 1 2 1 0 0 0 2 2 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 2 2 2 0]\n",
      "value function: [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.59  0.    0.    0.    0.    0.    0.    0.59  0.656\n",
      " 0.    0.    0.    0.    0.    0.    0.656 0.729 0.    0.    0.    0.\n",
      " 0.59  0.656 0.729 0.81  0.    0.    0.    0.59  0.656 0.729 0.    0.9\n",
      " 0.    0.    0.    0.    0.    0.81  0.    1.    0.    0.    0.    0.\n",
      " 0.81  0.9   1.    0.   ]\n",
      "======================7======================\n",
      "\n",
      "Policy: [0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 2 1 1 0 0 0 0 1 0 1 1 0 0 0 0 1\n",
      " 1 2 1 0 0 0 2 2 1 0 1 0 0 0 3 0 1 0 1 0 0 0 0 2 2 2 0]\n",
      "value function: [0.    0.    0.    0.    0.    0.    0.    0.531 0.    0.    0.    0.\n",
      " 0.    0.    0.531 0.59  0.    0.    0.    0.    0.    0.531 0.59  0.656\n",
      " 0.    0.    0.    0.    0.531 0.    0.656 0.729 0.    0.    0.    0.\n",
      " 0.59  0.656 0.729 0.81  0.    0.    0.    0.59  0.656 0.729 0.    0.9\n",
      " 0.    0.    0.    0.531 0.    0.81  0.    1.    0.    0.    0.    0.\n",
      " 0.81  0.9   1.    0.   ]\n",
      "======================8======================\n",
      "\n",
      "Policy: [0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 2 1 1 0 0 0 2 1 0 1 1 0 0 0 0 1\n",
      " 1 2 1 0 0 0 2 2 1 0 1 0 0 2 3 0 1 0 1 0 0 0 0 2 2 2 0]\n",
      "value function: [0.    0.    0.    0.    0.    0.    0.478 0.531 0.    0.    0.    0.\n",
      " 0.    0.478 0.531 0.59  0.    0.    0.    0.    0.478 0.531 0.59  0.656\n",
      " 0.    0.    0.    0.478 0.531 0.    0.656 0.729 0.    0.    0.    0.\n",
      " 0.59  0.656 0.729 0.81  0.    0.    0.    0.59  0.656 0.729 0.    0.9\n",
      " 0.    0.    0.478 0.531 0.    0.81  0.    1.    0.    0.    0.    0.\n",
      " 0.81  0.9   1.    0.   ]\n",
      "======================9======================\n",
      "\n",
      "Policy: [0 0 0 0 0 1 1 1 0 0 0 0 1 1 1 1 0 0 0 0 1 2 1 1 0 0 2 2 1 0 1 1 0 0 0 0 1\n",
      " 1 2 1 0 0 0 2 2 1 0 1 0 0 2 3 0 1 0 1 0 0 3 0 2 2 2 0]\n",
      "value function: [0.    0.    0.    0.    0.    0.43  0.478 0.531 0.    0.    0.    0.\n",
      " 0.43  0.478 0.531 0.59  0.    0.    0.    0.    0.478 0.531 0.59  0.656\n",
      " 0.    0.    0.43  0.478 0.531 0.    0.656 0.729 0.    0.    0.    0.\n",
      " 0.59  0.656 0.729 0.81  0.    0.    0.    0.59  0.656 0.729 0.    0.9\n",
      " 0.    0.    0.478 0.531 0.    0.81  0.    1.    0.    0.    0.43  0.\n",
      " 0.81  0.9   1.    0.   ]\n",
      "======================10======================\n",
      "\n",
      "Policy: [0 0 0 0 1 1 1 1 0 0 0 2 1 1 1 1 0 0 1 0 1 2 1 1 0 0 2 2 1 0 1 1 0 0 3 0 1\n",
      " 1 2 1 0 0 0 2 2 1 0 1 0 0 2 3 0 1 0 1 0 2 3 0 2 2 2 0]\n",
      "value function: [0.    0.    0.    0.    0.387 0.43  0.478 0.531 0.    0.    0.    0.387\n",
      " 0.43  0.478 0.531 0.59  0.    0.    0.387 0.    0.478 0.531 0.59  0.656\n",
      " 0.    0.    0.43  0.478 0.531 0.    0.656 0.729 0.    0.    0.387 0.\n",
      " 0.59  0.656 0.729 0.81  0.    0.    0.    0.59  0.656 0.729 0.    0.9\n",
      " 0.    0.    0.478 0.531 0.    0.81  0.    1.    0.    0.387 0.43  0.\n",
      " 0.81  0.9   1.    0.   ]\n",
      "======================11======================\n",
      "\n",
      "Policy: [0 0 0 1 1 1 1 1 0 0 1 2 1 1 1 1 0 2 1 0 1 2 1 1 0 0 2 2 1 0 1 1 0 2 3 0 1\n",
      " 1 2 1 0 0 0 2 2 1 0 1 0 0 2 3 0 1 0 1 2 2 3 0 2 2 2 0]\n",
      "value function: [0.    0.    0.    0.349 0.387 0.43  0.478 0.531 0.    0.    0.349 0.387\n",
      " 0.43  0.478 0.531 0.59  0.    0.349 0.387 0.    0.478 0.531 0.59  0.656\n",
      " 0.    0.    0.43  0.478 0.531 0.    0.656 0.729 0.    0.349 0.387 0.\n",
      " 0.59  0.656 0.729 0.81  0.    0.    0.    0.59  0.656 0.729 0.    0.9\n",
      " 0.    0.    0.478 0.531 0.    0.81  0.    1.    0.349 0.387 0.43  0.\n",
      " 0.81  0.9   1.    0.   ]\n",
      "======================12======================\n",
      "\n",
      "Policy: [0 0 1 1 1 1 1 1 0 1 1 2 1 1 1 1 2 2 1 0 1 2 1 1 0 0 2 2 1 0 1 1 2 2 3 0 1\n",
      " 1 2 1 0 3 0 2 2 1 0 1 1 0 2 3 0 1 0 1 2 2 3 0 2 2 2 0]\n",
      "value function: [0.    0.    0.314 0.349 0.387 0.43  0.478 0.531 0.    0.314 0.349 0.387\n",
      " 0.43  0.478 0.531 0.59  0.314 0.349 0.387 0.    0.478 0.531 0.59  0.656\n",
      " 0.    0.    0.43  0.478 0.531 0.    0.656 0.729 0.314 0.349 0.387 0.\n",
      " 0.59  0.656 0.729 0.81  0.    0.314 0.    0.59  0.656 0.729 0.    0.9\n",
      " 0.314 0.    0.478 0.531 0.    0.81  0.    1.    0.349 0.387 0.43  0.\n",
      " 0.81  0.9   1.    0.   ]\n",
      "======================13======================\n",
      "\n",
      "Policy: [0 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 2 2 1 0 1 2 1 1 1 0 2 2 1 0 1 1 2 2 3 0 1\n",
      " 1 2 1 1 3 0 2 2 1 0 1 1 0 2 3 0 1 0 1 2 2 3 0 2 2 2 0]\n",
      "value function: [0.    0.282 0.314 0.349 0.387 0.43  0.478 0.531 0.282 0.314 0.349 0.387\n",
      " 0.43  0.478 0.531 0.59  0.314 0.349 0.387 0.    0.478 0.531 0.59  0.656\n",
      " 0.282 0.    0.43  0.478 0.531 0.    0.656 0.729 0.314 0.349 0.387 0.\n",
      " 0.59  0.656 0.729 0.81  0.282 0.314 0.    0.59  0.656 0.729 0.    0.9\n",
      " 0.314 0.    0.478 0.531 0.    0.81  0.    1.    0.349 0.387 0.43  0.\n",
      " 0.81  0.9   1.    0.   ]\n",
      "======================14======================\n",
      "\n",
      "Policy: [1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 2 2 1 0 1 2 1 1 1 0 2 2 1 0 1 1 2 2 3 0 1\n",
      " 1 2 1 1 3 0 2 2 1 0 1 1 0 2 3 0 1 0 1 2 2 3 0 2 2 2 0]\n",
      "value function: [0.254 0.282 0.314 0.349 0.387 0.43  0.478 0.531 0.282 0.314 0.349 0.387\n",
      " 0.43  0.478 0.531 0.59  0.314 0.349 0.387 0.    0.478 0.531 0.59  0.656\n",
      " 0.282 0.    0.43  0.478 0.531 0.    0.656 0.729 0.314 0.349 0.387 0.\n",
      " 0.59  0.656 0.729 0.81  0.282 0.314 0.    0.59  0.656 0.729 0.    0.9\n",
      " 0.314 0.    0.478 0.531 0.    0.81  0.    1.    0.349 0.387 0.43  0.\n",
      " 0.81  0.9   1.    0.   ]\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Episode reward: 1.000000\n",
      "\n",
      "-------------------------\n",
      "Beginning Value Iteration\n",
      "-------------------------\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Episode reward: 1.000000\n",
      "\n",
      "-------------------------\n",
      "Beginning Policy Iteration\n",
      "-------------------------\n",
      "======================0======================\n",
      "\n",
      "Policy: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "value function: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "======================1======================\n",
      "\n",
      "Policy: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      "value function: [0.000e+00 0.000e+00 5.832e-04 0.000e+00 0.000e+00 0.000e+00 7.063e-03\n",
      " 0.000e+00 0.000e+00 0.000e+00 8.023e-02 0.000e+00 0.000e+00 0.000e+00\n",
      " 8.867e-01 0.000e+00]\n",
      "======================2======================\n",
      "\n",
      "Policy: [0 2 1 0 0 0 1 0 0 2 1 0 0 2 2 0]\n",
      "value function: [0.    0.352 0.446 0.352 0.    0.    0.533 0.    0.    0.606 0.74  0.\n",
      " 0.    0.813 0.952 0.   ]\n",
      "======================3======================\n",
      "\n",
      "Policy: [2 2 1 0 0 0 1 0 2 1 1 0 0 2 2 0]\n",
      "value function: [0.31  0.359 0.453 0.359 0.269 0.    0.54  0.    0.535 0.709 0.75  0.\n",
      " 0.    0.824 0.953 0.   ]\n",
      "======================4======================\n",
      "\n",
      "Policy: [2 2 1 0 1 0 1 0 2 1 1 0 0 2 2 0]\n",
      "value function: [0.326 0.358 0.453 0.358 0.435 0.    0.54  0.    0.551 0.711 0.75  0.\n",
      " 0.    0.825 0.953 0.   ]\n",
      "======================5======================\n",
      "\n",
      "Policy: [1 2 1 0 1 0 1 0 2 1 1 0 0 2 2 0]\n",
      "value function: [0.379 0.358 0.453 0.358 0.435 0.    0.54  0.    0.551 0.711 0.75  0.\n",
      " 0.    0.825 0.953 0.   ]\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Episode reward: 1.000000\n",
      "\n",
      "-------------------------\n",
      "Beginning Value Iteration\n",
      "-------------------------\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Rico: rendering\n",
      "Episode reward: 0.000000\n"
     ]
    }
   ],
   "source": [
    "def value_iteration(P, nS, nA, gamma=0.9, tol=1e-3):\n",
    "    \"\"\"\n",
    "    Learn value function and policy by using value iteration method for a given\n",
    "    gamma and environment.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    p = {a1: [(p1, s'1, r1, terminal) ...]}\n",
    "    P, nS, nA, gamma:\n",
    "        defined at beginning of file\n",
    "    tol: float\n",
    "        Terminate value iteration when\n",
    "            max |value_function(s) - prev_value_function(s)| < tol\n",
    "    Returns:\n",
    "    ----------\n",
    "    value_function: np.ndarray[nS]\n",
    "    policy: np.ndarray[nS]\n",
    "    \"\"\"\n",
    "    value_function = np.zeros(nS)\n",
    "    policy = np.zeros(nS, dtype=int)\n",
    "    should_continue = True\n",
    "    i = 0\n",
    "    with open(\"value_iteration_history.txt\", \"w\") as f:\n",
    "        while should_continue:\n",
    "            should_continue = False\n",
    "            for s in range(nS):\n",
    "                transitions = P[s]\n",
    "                for action, action_ps in transitions.items():\n",
    "                    v = 0\n",
    "                    for action_p in action_ps:\n",
    "                        p, s_prime, r, terminal = action_p\n",
    "                        # NOTE: we are getting expectation of total rewards\n",
    "                        v += p*(r + gamma * value_function[s_prime])\n",
    "                    next_value = max(value_function[s], v)\n",
    "                    # Pick the action that leads to the highest valued state.\n",
    "                    if next_value == v:\n",
    "                        policy[s] = action\n",
    "                    if not should_continue and abs(next_value - value_function[s]) > tol:\n",
    "                        should_continue = True\n",
    "                    value_function[s] = next_value\n",
    "            # f.write(f\"======================{i}======================\\n\")\n",
    "            # f.write(\"value function:\\n\")\n",
    "            # f.write(str(value_function)+\"\\n\")\n",
    "            # f.write(\"policy:\")\n",
    "            # f.write(str(policy) + \"\\n\")\n",
    "            i += 1\n",
    "        # for every s, find all its ps. and probabilities and next state\n",
    "    return value_function, policy\n",
    "\n",
    "def policy_evaluation(P, nS, nA, policy, gamma=0.9, tol=1e-3):\n",
    "    value_function = -100*np.ones(nS)\n",
    "    next_value_function = np.zeros(nS)\n",
    "    i = 0\n",
    "    while True:\n",
    "        if all(np.abs(next_value_function - value_function) < tol):\n",
    "            break\n",
    "        value_function = next_value_function\n",
    "        next_value_function = np.zeros(nS)\n",
    "        for s in range(nS):\n",
    "            action = policy[s]\n",
    "            for transition in P[s][action]:\n",
    "                p, next_state, r, done = transition\n",
    "                next_value_function[s] += p*(r + gamma*value_function[next_state])\n",
    "    return value_function\n",
    "\n",
    "def policy_update(P, value_function, nS, nA, gamma=0.9):\n",
    "    # Consider all expected r + gamma*V(s')\n",
    "    # Q(s,a) = E[r(s, a, s') + gamma*V(s')]\n",
    "    new_policy = np.zeros(nS, dtype=int)\n",
    "    Q_sas = np.zeros(nS)\n",
    "    for s in range(nS):\n",
    "        for action, transitions in P[s].items():\n",
    "            Q_sa = 0\n",
    "            for transition in transitions:\n",
    "                p, next_state, r, done = transition\n",
    "                Q_sa += p * (r + gamma * value_function[next_state])\n",
    "            if Q_sa > Q_sas[s]:\n",
    "                new_policy[s] = action\n",
    "                Q_sas[s] = Q_sa\n",
    "    return new_policy\n",
    "        \n",
    "\n",
    "def policy_iteration(P, nS, nA, gamma=0.9, tol=1e-3):\n",
    "    policy = np.zeros(nS)\n",
    "    i = 0\n",
    "    while True:\n",
    "        value_function = policy_evaluation(P, nS, nA, policy, gamma=gamma)\n",
    "        new_policy = policy_update(P, value_function, nS, nA, gamma=gamma)\n",
    "        #TODO Remember to remove\n",
    "        print(f\"======================{i}======================\\n\")\n",
    "        print(f'Policy: {policy}')\n",
    "        print(f'value function: {value_function}')\n",
    "        if all(np.abs(new_policy - policy) < tol):\n",
    "            break\n",
    "        policy = new_policy\n",
    "        i += 1\n",
    "    return value_function, policy\n",
    "\n",
    "    \n",
    "def render_single(env, policy, state_output_file, max_steps=100):\n",
    "    \"\"\"\n",
    "    env: gym.core.Environment - Environment to play on. Must have nS, nA, and P as attributes.\n",
    "    Policy: np.array of shape [env.nS]. The action to take at a given state\n",
    "    \"\"\" \n",
    "    episode_reward = 0\n",
    "    ob = env.reset()\n",
    "    try:\n",
    "        os.remove(state_output_file)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    for t in range(max_steps):\n",
    "        env.render(state_output_file)\n",
    "        a = policy[ob]\n",
    "        ob, rew, done, _ = env.step(a)\n",
    "        episode_reward += rew\n",
    "        if done:\n",
    "            break\n",
    "        time.sleep(0.25)\n",
    "    env.render(state_output_file)\n",
    "    if not done:\n",
    "        print(\"The agent didn't reach a terminal state in {} steps.\".format(max_steps))\n",
    "    else:\n",
    "        print(\"Episode reward: %f\" % episode_reward)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "Intro: Terminal State is 15, immediate state is 14. \n",
    "    - If there's no revisited states, and reward from goal state to goal state is 0,\n",
    "    then value iteration will simply polulate from end pose back. Because r + gamma * V(s') = r at the second last state, and for rest of the states, r + gamma * V, with constant V for all other states.\n",
    "    \n",
    "WHY DOES IT ALWAYS FIND THE OPTIMAL POLICIES AND VALUES?\n",
    "Let's call the bellman backup operator B, where $B^{\\pi}V = R(s,a) + yV^{\\pi}(s')$, following a policy $\\pi$\n",
    "- Does policy evaluation (see below) converge? Yes. One can prove $(BV1 - BV2 < |V1 - V2|_{\\infty})$. policy evaluation always converges\n",
    "- If you have two values functions that $V1(s) > V2(s)$, we can guarantee that $B^{\\pi}V1 > B^{\\pi}V2$\n",
    "- If you take evaluate your value function, by \"taking best action\" at the current iteration, we call it bellman optimality operator, which is $B^{*}V = max_{action} R(s,a) + yV^{\\pi}(s')$. We can prove: $B^{*}V > V$. That means, if you keep doing bellman optimality backup, your value function estimate will converge!\n",
    "- Now the question: does it converge to optimal value? \n",
    "    - We know the optimal value function must exist, because we have **A FINITE SET OF POLICIES**.\n",
    "    - Do we get the optimal value with bellman optimality backup? YES! Because $B^{*}V >= B^{\\pi}V$ for any policy, $\\pi$. So it's no worse than the best policy. and we know, the value function will converge.\n",
    "\n",
    "```\n",
    "======================0======================\n",
    "value function:\n",
    "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
    "policy:[3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3]\n",
    "======================1======================\n",
    "value function:\n",
    "[0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.9 0.  0.  0.9 1.  0. ]\n",
    "policy:[3 3 3 3 3 3 3 3 3 3 1 3 3 2 2 3]\n",
    "======================2======================\n",
    "value function:\n",
    "[0.   0.   0.   0.   0.   0.   0.81 0.   0.   0.81 0.9  0.   0.   0.9\n",
    " 1.   0.  ]\n",
    "policy:[3 3 3 3 3 3 1 3 3 2 1 3 3 2 2 3]\n",
    "======================3======================\n",
    "value function:\n",
    "[0.    0.    0.729 0.656 0.    0.    0.81  0.    0.729 0.81  0.9   0.\n",
    " 0.    0.9   1.    0.   ]\n",
    "policy:[3 3 1 0 3 3 1 3 2 2 1 3 3 2 2 3]\n",
    "======================4======================\n",
    "value function:\n",
    "[0.    0.656 0.729 0.656 0.656 0.    0.81  0.    0.729 0.81  0.9   0.\n",
    " 0.    0.9   1.    0.   ]\n",
    "policy:[3 2 1 0 1 3 1 3 2 2 1 3 3 2 2 3]\n",
    "======================5======================\n",
    "value function:\n",
    "[0.59  0.656 0.729 0.656 0.656 0.    0.81  0.    0.729 0.81  0.9   0.\n",
    " 0.    0.9   1.    0.   ]\n",
    "policy:[2 2 1 0 1 3 1 3 2 2 1 3 3 2 2 3]\n",
    "======================6======================\n",
    "value function:\n",
    "[0.59  0.656 0.729 0.656 0.656 0.    0.81  0.    0.729 0.81  0.9   0.\n",
    " 0.    0.9   1.    0.   ]\n",
    "policy:[2 2 1 0 1 3 1 3 2 2 1 3 3 2 2 3]\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration Results\n",
    "\n",
    "It's possible that initially, value function is zero everywhere. However, policy update can still break the tie because it considers expectation of total rewards, across all actions. \n",
    "Value iteration at the end will reach the final value function at optimal policy\n",
    "```\n",
    "======================0======================\n",
    "\n",
    "Policy: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "value function: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "======================1======================\n",
    "\n",
    "Policy: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0]\n",
    "value function: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
    "======================2======================\n",
    "\n",
    "Policy: [0 0 0 0 0 0 0 0 0 0 1 0 0 2 2 0]\n",
    "value function: [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.9 0.  0.  0.9 1.  0. ]\n",
    "======================3======================\n",
    "\n",
    "Policy: [0 0 0 0 0 0 1 0 0 1 1 0 0 2 2 0]\n",
    "value function: [0.   0.   0.   0.   0.   0.   0.81 0.   0.   0.81 0.9  0.   0.   0.9\n",
    " 1.   0.  ]\n",
    "======================4======================\n",
    "\n",
    "Policy: [0 0 1 0 0 0 1 0 2 1 1 0 0 2 2 0]\n",
    "value function: [0.    0.    0.729 0.656 0.    0.    0.81  0.    0.729 0.81  0.9   0.\n",
    " 0.    0.9   1.    0.   ]\n",
    "======================5======================\n",
    "\n",
    "Policy: [0 2 1 0 1 0 1 0 2 1 1 0 0 2 2 0]\n",
    "value function: [0.    0.656 0.729 0.656 0.656 0.    0.81  0.    0.729 0.81  0.9   0.\n",
    " 0.    0.9   1.    0.   ]\n",
    "======================6======================\n",
    "\n",
    "Policy: [1 2 1 0 1 0 1 0 2 1 1 0 0 2 2 0]\n",
    "value function: [0.59  0.656 0.729 0.656 0.656 0.    0.81  0.    0.729 0.81  0.9   0.\n",
    " 0.    0.9   1.    0.   ]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
