{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from frozen_lake_env import FrozenLakeEnv\n",
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rico: current state 14, new state: 15\n",
      "\n",
      "-------------------------\n",
      "Beginning Value Iteration\n",
      "-------------------------\n",
      "Rico: should continue\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "Rico: should continue\n",
      "[0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.9 0.  0.  0.9 1.  0. ]\n",
      "Rico: should continue\n",
      "[0.   0.   0.   0.   0.   0.   0.81 0.   0.   0.81 0.9  0.   0.   0.9\n",
      " 1.   0.  ]\n",
      "Rico: should continue\n",
      "[0.    0.    0.729 0.656 0.    0.    0.81  0.    0.729 0.81  0.9   0.\n",
      " 0.    0.9   1.    0.   ]\n",
      "Rico: should continue\n",
      "[0.    0.656 0.729 0.656 0.656 0.    0.81  0.    0.729 0.81  0.9   0.\n",
      " 0.    0.9   1.    0.   ]\n",
      "Rico: should continue\n",
      "[0.59  0.656 0.729 0.656 0.656 0.    0.81  0.    0.729 0.81  0.9   0.\n",
      " 0.    0.9   1.    0.   ]\n",
      "[0.59  0.656 0.729 0.656 0.656 0.    0.81  0.    0.729 0.81  0.9   0.\n",
      " 0.    0.9   1.    0.   ]\n",
      "Episode reward: 1.000000\n"
     ]
    }
   ],
   "source": [
    "# Glossary\n",
    "# P: nested dictionary\n",
    "# \tFrom gym.core.Environment\n",
    "# \tFor each pair of states in [1, nS] and actions in [1, nA], P[state][action] is a\n",
    "# \ttuple of the form (probability, nextstate, reward, terminal) where\n",
    "# \t\t- probability: float\n",
    "# \t\t\tthe probability of transitioning from \"state\" to \"nextstate\" with \"action\"\n",
    "# \t\t- nextstate: int\n",
    "# \t\t\tdenotes the state we transition to (in range [0, nS - 1])\n",
    "# \t\t- reward: int\n",
    "# \t\t\teither 0 or 1, the reward for transitioning from \"state\" to\n",
    "# \t\t\t\"nextstate\" with \"action\"\n",
    "# \t\t- terminal: bool\n",
    "# \t\t  True when \"nextstate\" is a terminal state (hole or goal), False otherwise\n",
    "# nS: int\n",
    "# \tnumber of states in the environment\n",
    "# nA: int\n",
    "# \tnumber of actions in the environment\n",
    "# gamma: float\n",
    "# \tDiscount factor. Number in range [0, 1)\n",
    "# Returns: index of action\n",
    "\n",
    "def policy_update(P, policy, nS, nA, gamma=0.9):\n",
    "    pass\n",
    "\n",
    "def policy_evaluation(P, nS, nA, policy, gamma=0.9, tol=1e-3):\n",
    "    pass\n",
    "\n",
    "def policy_iteration(P, nS, nA, gamma=0.9, tol=1e-3):\n",
    "    pass\n",
    "\n",
    "def value_iteration(P, nS, nA, gamma=0.9, tol=1e-3):\n",
    "    \"\"\"\n",
    "    Learn value function and policy by using value iteration method for a given\n",
    "    gamma and environment.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    p = {a1: [(p1, s'1, r1, terminal) ...]}\n",
    "    P, nS, nA, gamma:\n",
    "        defined at beginning of file\n",
    "    tol: float\n",
    "        Terminate value iteration when\n",
    "            max |value_function(s) - prev_value_function(s)| < tol\n",
    "    Returns:\n",
    "    ----------\n",
    "    value_function: np.ndarray[nS]\n",
    "    policy: np.ndarray[nS]\n",
    "    \"\"\"\n",
    "    value_function = np.zeros(nS)\n",
    "    policy = np.zeros(nS, dtype=int)\n",
    "    should_continue = True\n",
    "    i = 0\n",
    "    with open(\"value_iteration_history.txt\", \"w\") as f:\n",
    "        while True:\n",
    "            if not should_continue:\n",
    "                break\n",
    "            should_continue = False\n",
    "            for s in range(nS):\n",
    "                transitions = P[s]\n",
    "                for action, action_ps in transitions.items():\n",
    "                    v = 0\n",
    "                    for action_p in action_ps:\n",
    "                        p, s_prime, r, terminal = action_p\n",
    "                        # NOTE: we are getting expectation of total rewards\n",
    "                        v += p*(r + gamma * value_function[s_prime])\n",
    "                    next_value = max(value_function[s], v)\n",
    "                    # Pick the action that leads to the highest valued state.\n",
    "                    if next_value == v:\n",
    "                        policy[s] = action\n",
    "                    if not should_continue and abs(next_value - value_function[s]) > tol:\n",
    "                        should_continue = True\n",
    "                    value_function[s] = next_value\n",
    "            #TODO Remember to remove\n",
    "            print(f'{value_function}')\n",
    "                \n",
    "            f.write(f\"======================{i}======================\\n\")\n",
    "            f.write(\"value function:\\n\")\n",
    "            f.write(str(value_function)+\"\\n\")\n",
    "            f.write(\"policy:\")\n",
    "            f.write(str(policy) + \"\\n\")\n",
    "            i += 1\n",
    "        # for every s, find all its ps. and probabilities and next state\n",
    "    \n",
    "\n",
    "    return value_function, policy\n",
    "\n",
    "def render_single(env, policy, max_steps=100):\n",
    "    \"\"\"\n",
    "    env: gym.core.Environment - Environment to play on. Must have nS, nA, and P as attributes.\n",
    "    Policy: np.array of shape [env.nS]. The action to take at a given state\n",
    "    \"\"\" \n",
    "    episode_reward = 0\n",
    "    ob = env.reset()\n",
    "    for t in range(max_steps):\n",
    "        env.render(\"frozen_lake_output.tmp\")\n",
    "        a = policy[ob]\n",
    "        ob, rew, done, _ = env.step(a)\n",
    "        episode_reward += rew\n",
    "        if done:\n",
    "            break\n",
    "        time.sleep(0.25)\n",
    "    env.render(\"frozen_lake_output.tmp\")\n",
    "    if not done:\n",
    "        print(\"The agent didn't reach a terminal state in {} steps.\".format(max_steps))\n",
    "    else:\n",
    "        print(\"Episode reward: %f\" % episode_reward)\n",
    "\n",
    "\n",
    "envs = [\n",
    "    FrozenLakeEnv(map_name=\"4x4\", is_slippery=False), \n",
    "    # FrozenLakeEnv(map_name=\"8x8\", is_slippery=False), \n",
    "    # FrozenLakeEnv(map_name=\"4x4\", is_slippery=True)\n",
    "    ]\n",
    "for env in envs:\n",
    "    # print(\"\\n\" + \"-\"*25 + \"\\nBeginning Policy Iteration\\n\" + \"-\"*25)\n",
    "    # V_pi, p_pi = policy_iteration(env.P, env.observation_space.n, env.action_space.n, gamma=0.9, tol=1e-3)\n",
    "    # render_single(env, p_pi, 100)\n",
    "\n",
    "    print(\"\\n\" + \"-\"*25 + \"\\nBeginning Value Iteration\\n\" + \"-\"*25)\n",
    "    V_vi, p_vi = value_iteration(env.P, env.nS, env.nA, gamma=0.9, tol=1e-3)\n",
    "    render_single(env, p_vi, 100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "Intro: Terminal State is 15, immediate state is 14. \n",
    "    - If there's no revisited states, and reward from goal state to goal state is 0,\n",
    "    then value iteration will simply polulate from end pose back. Because r + gamma * V(s') = r at the second last state, and for rest of the states, r + gamma * V, with constant V for all other states.\n",
    "    - What about revisited states?\n",
    "\n",
    "```\n",
    "======================0======================\n",
    "value function:\n",
    "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
    "policy:[3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3]\n",
    "======================1======================\n",
    "value function:\n",
    "[0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.9 0.  0.  0.9 1.  0. ]\n",
    "policy:[3 3 3 3 3 3 3 3 3 3 1 3 3 2 2 3]\n",
    "======================2======================\n",
    "value function:\n",
    "[0.   0.   0.   0.   0.   0.   0.81 0.   0.   0.81 0.9  0.   0.   0.9\n",
    " 1.   0.  ]\n",
    "policy:[3 3 3 3 3 3 1 3 3 2 1 3 3 2 2 3]\n",
    "======================3======================\n",
    "value function:\n",
    "[0.    0.    0.729 0.656 0.    0.    0.81  0.    0.729 0.81  0.9   0.\n",
    " 0.    0.9   1.    0.   ]\n",
    "policy:[3 3 1 0 3 3 1 3 2 2 1 3 3 2 2 3]\n",
    "======================4======================\n",
    "value function:\n",
    "[0.    0.656 0.729 0.656 0.656 0.    0.81  0.    0.729 0.81  0.9   0.\n",
    " 0.    0.9   1.    0.   ]\n",
    "policy:[3 2 1 0 1 3 1 3 2 2 1 3 3 2 2 3]\n",
    "======================5======================\n",
    "value function:\n",
    "[0.59  0.656 0.729 0.656 0.656 0.    0.81  0.    0.729 0.81  0.9   0.\n",
    " 0.    0.9   1.    0.   ]\n",
    "policy:[2 2 1 0 1 3 1 3 2 2 1 3 3 2 2 3]\n",
    "======================6======================\n",
    "value function:\n",
    "[0.59  0.656 0.729 0.656 0.656 0.    0.81  0.    0.729 0.81  0.9   0.\n",
    " 0.    0.9   1.    0.   ]\n",
    "policy:[2 2 1 0 1 3 1 3 2 2 1 3 3 2 2 3]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
